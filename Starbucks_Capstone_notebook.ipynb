{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starbucks Capstone Challenge\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This data set contains simulated data that mimics customer behavior on the Starbucks rewards mobile app. Once every few days, Starbucks sends out an offer to users of the mobile app. An offer can be merely an advertisement for a drink or an actual offer such as a discount or BOGO (buy one get one free). Some users might not receive any offer during certain weeks. \n",
    "\n",
    "Not all users receive the same offer, and that is the challenge to solve with this data set.\n",
    "\n",
    "Your task is to combine transaction, demographic and offer data to determine which demographic groups respond best to which offer type. This data set is a simplified version of the real Starbucks app because the underlying simulator only has one product whereas Starbucks actually sells dozens of products.\n",
    "\n",
    "Every offer has a validity period before the offer expires. As an example, a BOGO offer might be valid for only 5 days. You'll see in the data set that informational offers have a validity period even though these ads are merely providing information about a product; for example, if an informational offer has 7 days of validity, you can assume the customer is feeling the influence of the offer for 7 days after receiving the advertisement.\n",
    "\n",
    "You'll be given transactional data showing user purchases made on the app including the timestamp of purchase and the amount of money spent on a purchase. This transactional data also has a record for each offer that a user receives as well as a record for when a user actually views the offer. There are also records for when a user completes an offer. \n",
    "\n",
    "Keep in mind as well that someone using the app might make a purchase through the app without having received an offer or seen an offer.\n",
    "\n",
    "### Example\n",
    "\n",
    "To give an example, a user could receive a discount offer buy 10 dollars get 2 off on Monday. The offer is valid for 10 days from receipt. If the customer accumulates at least 10 dollars in purchases during the validity period, the customer completes the offer.\n",
    "\n",
    "However, there are a few things to watch out for in this data set. Customers do not opt into the offers that they receive; in other words, a user can receive an offer, never actually view the offer, and still complete the offer. For example, a user might receive the \"buy 10 dollars get 2 dollars off offer\", but the user never opens the offer during the 10 day validity period. The customer spends 15 dollars during those ten days. There will be an offer completion record in the data set; however, the customer was not influenced by the offer because the customer never viewed the offer.\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "This makes data cleaning especially important and tricky.\n",
    "\n",
    "You'll also want to take into account that some demographic groups will make purchases even if they don't receive an offer. From a business perspective, if a customer is going to make a 10 dollar purchase without an offer anyway, you wouldn't want to send a buy 10 dollars get 2 dollars off offer. You'll want to try to assess what a certain demographic group will buy when not receiving any offers.\n",
    "\n",
    "### Final Advice\n",
    "\n",
    "Because this is a capstone project, you are free to analyze the data any way you see fit. For example, you could build a machine learning model that predicts how much someone will spend based on demographics and offer type. Or you could build a model that predicts whether or not someone will respond to an offer. Or, you don't need to build a machine learning model at all. You could develop a set of heuristics that determine what offer you should send to each customer (i.e., 75 percent of women customers who were 35 years old responded to offer A vs 40 percent from the same demographic to offer B, so send offer A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets\n",
    "\n",
    "The data is contained in three files:\n",
    "\n",
    "* portfolio.json - containing offer ids and meta data about each offer (duration, type, etc.)\n",
    "* profile.json - demographic data for each customer\n",
    "* transcript.json - records for transactions, offers received, offers viewed, and offers completed\n",
    "\n",
    "Here is the schema and explanation of each variable in the files:\n",
    "\n",
    "**portfolio.json**\n",
    "* id (string) - offer id\n",
    "* offer_type (string) - type of offer ie BOGO, discount, informational\n",
    "* difficulty (int) - minimum required spend to complete an offer\n",
    "* reward (int) - reward given for completing an offer\n",
    "* duration (int) - time for offer to be open, in days\n",
    "* channels (list of strings)\n",
    "\n",
    "**profile.json**\n",
    "* age (int) - age of the customer \n",
    "* became_member_on (int) - date when customer created an app account\n",
    "* gender (str) - gender of the customer (note some entries contain 'O' for other rather than M or F)\n",
    "* id (str) - customer id\n",
    "* income (float) - customer's income\n",
    "\n",
    "**transcript.json**\n",
    "* event (str) - record description (ie transaction, offer received, offer viewed, etc.)\n",
    "* person (str) - customer id\n",
    "* time (int) - time in hours since start of test. The data begins at time t=0\n",
    "* value - (dict of strings) - either an offer id or transaction amount depending on the record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Importing libaries & dataset](#data)\n",
    "2. Explore and clean data :\n",
    "    [portfolio](#portfolio)\n",
    "    [profile](#profile)\n",
    "    [transcript](#transcript)\n",
    "3. [Exploratory analysis: Part1](#explore-part1)\n",
    "4. [Exploratory analysis: Part2](#explore-part2)\n",
    "5. [Feature engineering](#feature)\n",
    "6. [Modeling](#model)\n",
    "7. [Conclusion](#conclude)\n",
    "\n",
    "\n",
    "<font color=\"orangered\">Jump right into the <strong> 3. Exporatory Analysis</strong> part if you want to skip the data cleaning and </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# `SECTION 1`\n",
    "\n",
    "<a id=\"modules\"></a>\n",
    "## 1a. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "import os.path\n",
    "import pickle\n",
    "from tqdm import tqdm # progress bar\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False # autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[to the current workspace](#work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the json files\n",
    "portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)\n",
    "profile = pd.read_json('data/profile.json', orient='records', lines=True)\n",
    "transcript = pd.read_json('data/transcript.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# `SECTION 2` Exploring & cleaning data\n",
    "\n",
    "There are three dataset to explore and clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"portfolio\"></a>\n",
    "## 2a. portfolio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "There are only 10 offers available in the data as it is a simplified version. \n",
    "\n",
    "For cleaning, note the following:\n",
    "- no missing value exists\n",
    "- date types look okay\n",
    "- need to destructure `channels` column as it is list <font color=\"orangered\">(to do)</font>\n",
    "- just confirm if there is any duplicates <font color=\"orangered\">(to do)</font>\n",
    "\n",
    "After cleaning, I would like to return:\n",
    "- a grouped by table that shows summary statistics of offers by `offer_type`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean `portfolio` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_portfolio(df):\n",
    "    ''' Clean the 'portolio' dataset, achieving :\n",
    "    1. Destructure 'channels' column\n",
    "    2. Drop 'channels' column after destructing\n",
    "    \n",
    "    INPUT: a Pandas dataframe that contains portfolio data\n",
    "    OUTPUT: a clean dataframe \n",
    "    '''\n",
    "    \n",
    "    # Get unique channels \n",
    "    temp_list = list()\n",
    "\n",
    "    for _type in df.channels:\n",
    "        temp_list.extend(_type)\n",
    "\n",
    "    channel_list = set(temp_list)\n",
    "    \n",
    "    \n",
    "    # Create binary tables for channels\n",
    "    channel_bool = dict()\n",
    "\n",
    "    for _type in channel_list:\n",
    "        channel_bool[_type] = df.channels.apply(lambda x: _type in x)\n",
    "    \n",
    "    channel_df = pd.DataFrame(channel_bool)\n",
    "    \n",
    "    \n",
    "    # Transform boolean to binary integer (True: 1, False: 0)\n",
    "    binary_rule = {True: 1, False: 0}\n",
    "    \n",
    "    for col in channel_df.columns:\n",
    "        channel_df[col] = channel_df[col].map(binary_rule) \n",
    "       \n",
    "    # Join the binary columns to the original dataframe\n",
    "    df_clean = df.join(channel_df)\n",
    "    \n",
    "    # Drop 'channels' columns\n",
    "    df_clean = df_clean.drop(columns='channels')\n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean portfolio dataframe\n",
    "portfolio_v1 = clean_portfolio(portfolio)\n",
    "portfolio_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "assert portfolio_v1.duplicated().sum() == 0, \"There is at least one duplicated offer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing stats table including channels\n",
    "portfolio_v1.groupby('offer_type').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "Customers gets the most reward from `bogo` it is less difficult to redeem than `discount` which requires more spends. \n",
    "\n",
    "When sending out offers, `email` is opted always. `bogo` looks the most aggressive offer utilising the 4 channels the most (just as it sounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the cleand dataframe as 'portfolio_v1'\n",
    "portfolio_v1.to_csv('data/portfolio_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"profile\"></a>\n",
    "## 2b. profile dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates by id\n",
    "assert profile.duplicated(subset='id').sum() == 0, \"There is at least one duplicated record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2175 records with null data \n",
    "# age of 118 is odd so to remove\n",
    "print(profile[profile.gender.isnull()].head(3))\n",
    "profile[profile.gender.isnull()].age.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "There is no duplicated id. `profile` data contains all unique user ids.\n",
    "\n",
    "For cleaning, note the following:\n",
    "- 2175 missing value exists in `gender`, `income` columns: contain age of 118 which is abnormal <font color=\"orangered\">(drop them) - 12.7% of the data</font>\n",
    "- `become_member_on` to be converted to datetime object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean `profile` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_profile(df):\n",
    "    ''' Clean the 'portolio' dataset, achieving :\n",
    "    1. Drop nulls\n",
    "    2. Convert 'become_member_on' to datetime object\n",
    "    \n",
    "    INPUT: a Pandas dataframe that contains profile data\n",
    "    OUTPUT: a clean dataframe \n",
    "    '''\n",
    "    \n",
    "    # Copy the original dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Drop nulls\n",
    "    df_clean = df_clean[df_clean.gender.notnull()]\n",
    "    \n",
    "    # Convert to datetime object\n",
    "    df_clean.loc[:, 'became_member_on'] = pd.to_datetime(df_clean.became_member_on, format=\"%Y%m%d\")\n",
    "    \n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining null data \n",
    "assert profile_v1.shape[0] == profile.shape[0] - 2175, \"It looks that missing values are not dropped correctly\"\n",
    "assert profile_v1.isnull().sum().sum() == 0, \"There still is missing value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clean data as profile_v1\n",
    "profile_v1.to_csv('data/profile_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transcript\"></a>\n",
    "## 2c. transcript dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transcript.event.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick inspection of the `value` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destructoring dictionary in 'value' column \n",
    "temp = transcript.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract unique keys from the dictionary\n",
    "value_keys = []\n",
    "\n",
    "for _dict in temp.value:\n",
    "    for keys in _dict.keys():\n",
    "        value_keys.append(keys)\n",
    "        \n",
    "print(set(value_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "\n",
    "There are 3 unique keys in the dictionary, while `offer_id` and `offer id` should give the same information. \n",
    "\n",
    "- We need destructuring the dictionary first \n",
    "- Then combine `offer_id` and `offer id` column together into `offer_id` column\n",
    "- drop `offer id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick way to destructure using 'json_normalize' method\n",
    "value_destructured = pd.json_normalize(temp['value'])\n",
    "value_destructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'offer_id' and 'offer id' columns \n",
    "# np.where(condition, value if true, value if false)\n",
    "value_destructured['offer_id'] = np.where(value_destructured['offer id'].notnull(), \n",
    "                                          value_destructured['offer id'], \n",
    "                                          value_destructured['offer_id'])\n",
    "\n",
    "# Drop the unnecessary offer id column if merger successful\n",
    "assert value_destructured['offer_id'].notnull().sum() == 167581, \"It is an incorrect merge\"\n",
    "value_destructured = value_destructured.drop(columns='offer id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting amount column\n",
    "print('Number of transactions made: ', value_destructured.amount.notnull().sum(), end='\\n\\n')\n",
    "print(value_destructured.amount.describe())\n",
    "\n",
    "sns.histplot(x=value_destructured.amount, binwidth=5); # every $5\n",
    "plt.title('Distribution of transaction amounts');\n",
    "plt.xlim([0,100]); # removing outliers i.e. max value of $1062.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transation value over $100 (hidden from the graph above)\n",
    "print(f'#trans over $100: {(value_destructured.amount > 100).sum()}')\n",
    "print(f'  in percentage: {(value_destructured.amount > 100).sum() / 138953 * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the amounts fall below $100 with some outliers that are minor enough to be ignored. Note that transaction values here includes those purchased with no offers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting reward column\n",
    "print('Total number of rewards given:', len(value_destructured.reward.notnull()))\n",
    "\n",
    "sns.countplot(x=value_destructured.reward);\n",
    "plt.title('Rewards given for completing offers and their counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards are given for offers that are completed. Therefore, it must correspond to `offer completed` events only. Reward 5 has been given the most followed by reward 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary - `transcript` data:\n",
    "\n",
    "There are four types of event following marketing funnels. As mentioned earlier an offer can be completed without receiving / viewing offer. Also, transaction can happen with no offer redeemed.\n",
    "\n",
    "No missing data is found in `transcript` data. However, after destructuring `value` column many null values are generated, which is to address no transactions / rewards and delivers a correct picture - no need to drop.\n",
    " \n",
    "For cleaning, \n",
    "- destructor dictionary in `value` column <font color=\"orangered\">(to do)</font>\n",
    "- make the keys of the ditionary in `value` column consistent <font color=\"orangered\">(to do)</font>\n",
    "- drop the original `value` column <font color=\"orangered\">(to do)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean `transcript` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transcript(df):\n",
    "    ''' Clean the 'transcript' dataset, achieving :\n",
    "    1. Destructure dictionary in 'value' column & create a new dataframe\n",
    "    2. Ensure consistency by merging offer_id with offer id column\n",
    "    3. Drop the duplicated offer id column\n",
    "    3. Join the new dataframe to the original transcript dataframe\n",
    "    4. Drop the original 'value' column\n",
    "\n",
    "    INPUT: a Pandas dataframe that contains transcript data\n",
    "    OUTPUT: a clean dataframe \n",
    "    '''\n",
    "    \n",
    "    # Instantiate df_clean as a copy of df\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Destructoring dictionary in 'value' column \n",
    "    value_df = pd.json_normalize(df.value)\n",
    "    \n",
    "    # Making values in 'value_key' column consistent + testing the result\n",
    "    # 'offer id' column merged 'offer_id' & test the change\n",
    "\n",
    "    # For testing, the merged column size should match sum of count 1 & 2\n",
    "    count1 = value_df['offer_id'].notnull().sum()\n",
    "    count2 = value_df['offer id'].notnull().sum()\n",
    "    \n",
    "    value_df['offer_id'] = np.where(value_df['offer id'].notnull(), \n",
    "                                    value_df['offer id'], value_df['offer_id'])\n",
    "    \n",
    "    # Testing if merged successfully\n",
    "    assert value_df['offer_id'].notnull().sum() == count1 + count2\n",
    "    \n",
    "    # Drop offer id column\n",
    "    value_df = value_df.drop(columns='offer id')\n",
    "\n",
    "    df_clean = df_clean.join(value_df)\n",
    "    \n",
    "    # Drop the original 'value' column\n",
    "    df_clean = df_clean.drop(columns='value')\n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the clean dataframe\n",
    "transcript_v1 = clean_transcript(transcript)\n",
    "transcript_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post cleaning inspection for `transcript_v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is a value in 'amount', it should be about transaction\n",
    "assert transcript_v1[transcript_v1.amount.notnull()].event.unique()[0] == 'transaction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for duplicates by 'person' and 'value_key'\n",
    "transript_v1_duplicated = transcript_v1[transcript_v1.duplicated(keep=False)]\n",
    "transript_v1_duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated event \n",
    "transript_v1_duplicated.event.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection for duplicates with a sample \n",
    "sample = transcript_v1[transcript_v1.person == 'b7e216b6472b46648272c29a52a86702']\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary\n",
    "About 397 fields (793 rows / 2) have duplicated records and it happends for `offer completed` event only. \n",
    "\n",
    "The above sample data shows the same type of duplication (idx# 305550, 305551). The duplicated fields do not really convery any meaningful data so drop duplicates (keep first)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates in `transcript_v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Before dropping duplicates: {transcript_v1.shape[0]} records')\n",
    "print(f'Total {transcript_v1.duplicated().sum()} duplicates exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_v1 = transcript_v1.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the changes\n",
    "assert transcript_v1.shape[0] == 306534 - 397, \"There is something wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clean data as transcript_v1\n",
    "transcript_v1.to_csv('data/transcript_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. transcript_v1 - further cleaning needed!\n",
    "Before any explatory analysis, we need a further cleaning of `transcript_v1` dataframe in order to clearly identify how funnels work and customers react to offers.\n",
    "\n",
    "As instructed in the introdution, transactions can be made without customers actually viewing offers, which does not really tell that customers were influenced by the offers.\n",
    "\n",
    "Therefore, we need to work on:\n",
    "1. Make `event` column categorical ordered\n",
    "2. Fill NaN value in `offer_id` for `transaction` made as a result of offer completion\n",
    "3. Pivot `transcript_v1` to see which offer / person completed offer or not\n",
    "\n",
    "The explanation here may not sound clear. So, I will explain the work along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_v1 = pd.read_csv('data/transcript_v1.csv')\n",
    "portfolio_v1 = pd.read_csv('data/portfolio_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-order event\n",
    "It is understood that when an offer is completed, it follows funnles like below:  \n",
    "    \n",
    "    'offer received' -> 'offer viewed' ->'transaction' -> 'offer completed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orangered\">It is important to note that there are two offers that are informational. These two offers have only two events: offer received and offer viewed. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get offer_id for the two information offer types\n",
    "information_ids = portfolio_v1[portfolio_v1['offer_type'] == 'informational']['id'].to_list()\n",
    "information_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events for informational offers\n",
    "transcript_v1[transcript_v1['offer_id'].isin(information_ids)]['event'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of analysis, we transform `event` column categorical, ordered following funnels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform event column into categorical ordered dtype\n",
    "funnel_order = ['offer received', 'offer viewed', 'transaction', 'offer completed']\n",
    "transcript_v1.event = pd.Categorical(transcript_v1.event, categories=funnel_order, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the dataframe by person, offer_id and then event\n",
    "# will represent dataframe in order of event(funnel) for each offer\n",
    "transcript_sorted = transcript_v1.sort_values(by=['person', 'offer_id', 'time', 'event'],\n",
    "                                              ascending=True)\n",
    "transcript_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later reference, create a function that facilitates the search of transaction details by a person's id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_details(_id, df=transcript_v1):\n",
    "    ''' To faciliate the search of transaction details of a customer.\n",
    "    Firstly sorting \n",
    "    \n",
    "    INPUT: \n",
    "        person_id: an id of a customer in the dataset\n",
    "        df: default as the cleaned transcript (v1) dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Copy the input dataframe\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Transform event column into categorical ordered dtype\n",
    "    funnel_order = ['offer received', 'offer viewed', 'transaction', 'offer completed']\n",
    "    df_new.event = pd.Categorical(df_new.event, categories=funnel_order, ordered=True)\n",
    "    \n",
    "    # Get the data that corresponds to the input customer id\n",
    "    df_ind = df_new[df_new.person == _id]\n",
    "    \n",
    "    return df_ind\n",
    "    \n",
    "    # Sorting the dataframe by offer_id, time and then event\n",
    "    # will represent dataframe in order of event(funnel) for each offer\n",
    "    df_ind = df_ind.sort_values(by=['offer_id', 'time', 'event'])\n",
    "\n",
    "    return df_ind                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_details('ffff82501cea40309d5fdd7edcca4a07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "The below method correctly sorts each offer by order of funnel. The rows with `transaction` does not contain offer_id. Therefore they need to be split and merged back after labeling complete offer. They can be merged on `time` of offer completed event which is exactly same as time of corresponding transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting rows that contain `transaction` event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows with 'transaction' event\n",
    "transaction_df = transcript_sorted[transcript_sorted.event == 'transaction']\n",
    "\n",
    "# Remove transaction from transcript_sorted dataframe\n",
    "transcript_sorted = transcript_sorted[transcript_sorted.event != 'transaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the change\n",
    "assert (transcript_sorted.event == 'transaction').sum() == 0, \"There still is a row with transaction event.\"\n",
    "assert (transcript_sorted.offer_id.isnull().sum()) == 0, \"There is a missing record in offer_id column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot transcript_sorted to label offer complete / incomplete\n",
    "In order to make pivoting less complex, `reward` column (where event == `offer completed`) will be put aside. It will later be added after labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting reward (offer completed)\n",
    "reward_df = transcript_v1[transcript_v1.event == 'offer completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remove reward column and subset transcript sorted dataframe\n",
    "transcript_subset = transcript_sorted[['person', 'offer_id', 'time', 'event']]\n",
    "\n",
    "# Pivot the subsetted dataframe \n",
    "# Group by and return the minimum time (the earliest) value \n",
    "offer_by_customer = transcript_subset.groupby(['person', 'offer_id', 'event']).min().unstack()\n",
    "offer_by_customer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "The pivoted dataframe `is_offer_complete` contains multi-layered index and column. It contains all 10 offer types (represented by `offer_id`) per each person. \n",
    "\n",
    "When it comes to column, `time` and `event` features are multi-layered, which will need a further processing in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect if all persons have 10 offers using progress bar\n",
    "\n",
    "member_ids = transcript_subset.person\n",
    "\n",
    "for _id in tqdm(member_ids):\n",
    "    assert offer_by_customer.loc[_id].shape[0] == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes indeed! This will allow us to check which offer has actually been sent and which hasn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset multi-index and drop 'transaction' column\n",
    "The multi-index column can be reset. Also `transaction` is regenerated as it is part of the categorical `event` column. To faciliate the labeling, we can drop it. Also, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting multi-index column\n",
    "offer_by_customer.columns = offer_by_customer.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if transaction column has any non-null value\n",
    "assert offer_by_customer['transaction'].notnull().sum() == 0, 'There is a non-null value in transaction column'\n",
    "\n",
    "# Drop the transaction column\n",
    "offer_by_customer = offer_by_customer.drop(columns='transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "offer_by_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling completed / incompleted offer\n",
    "Now we have **offer received**, **offer viewed** and **offer completed** column following funnel. We label `completed` when all three columns are filled with non-null values. \n",
    "\n",
    "**Labeling matrix for possible mix (o: non-null value, x: null):**\n",
    "\n",
    "| offer received | offer viewed | offer completed | label |\n",
    "| --- | --- | --- | --- |\n",
    "| o | o | o | completed |\n",
    "| o | o | x | incomplete |\n",
    "| o | x | o | incomplete |\n",
    "| o | x | x | incomplete |\n",
    "| x | x | x | unsent |\n",
    "\n",
    "As inspected earlier, the dataframe contains data for offers that have not been sent to a person and we denote that as `unsent` for any future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_offer_by_customer(df=offer_by_customer):\n",
    "    ''' Create a new dataframe that contains label column\n",
    "    to represent the status of offer. The column will contain\n",
    "    three labels that are 'completed', 'incomplete', 'no offer'\n",
    "    \n",
    "        'completed': customers has been influenced by offer when\n",
    "            purchasing a product\n",
    "        'incomplete': customers may have purchased a product or not\n",
    "            which however was not influenced by offer\n",
    "        'unsent': a particular offer has not been sent by Starbucks            \n",
    "    \n",
    "    INPUT: a pivoted dataframe that contains funnel information by\n",
    "        customer(person) and offer(offer_id)\n",
    "        \n",
    "    OUTPUT: a dataframe that has either of the three labels attached    \n",
    "    '''\n",
    "    \n",
    "    # Instantiate a copy of dataframe\n",
    "    df_new = offer_by_customer.copy()\n",
    "\n",
    "    # Change dtype categorical --> str\n",
    "    # otherwise, a new column cannot be added\n",
    "    df_new.columns = df_new.columns.astype(str)\n",
    "\n",
    "    # Instantiate a new 'offer_status' column\n",
    "    df_new['offer_status'] = ''\n",
    "    \n",
    "    for idx, rows in tqdm(df_new.iterrows(), total=df_new.shape[0]):\n",
    "    \n",
    "        # Set conditions to check if there is non-null value\n",
    "        is_received = not math.isnan(rows['offer received'])\n",
    "        is_viewed = not math.isnan(rows['offer viewed'])\n",
    "        is_completed = not math.isnan(rows['offer completed'])\n",
    "\n",
    "        # Label as per the logic spcified in the labeling matrix \n",
    "        if is_received and is_viewed and is_completed:\n",
    "            df_new.loc[idx, 'offer_status'] = 'completed'\n",
    "\n",
    "        elif is_received and is_viewed and not is_completed:\n",
    "            df_new.loc[idx, 'offer_status'] = 'incomplete'\n",
    "\n",
    "        elif is_received and not is_viewed and is_completed:\n",
    "            df_new.loc[idx, 'offer_status'] = 'incomplete'\n",
    "\n",
    "        elif is_received and not is_viewed and not is_completed:\n",
    "            df_new.loc[idx, 'offer_status'] = 'incomplete'\n",
    "\n",
    "        elif not is_received and not is_completed and not is_viewed:\n",
    "            df_new.loc[idx, 'offer_status'] = 'unsent'\n",
    "\n",
    "        else:\n",
    "            df_new.loc[idx, 'offer_status'] = np.nan\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_by_customer_labeled = label_offer_by_customer(df=offer_by_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if offer_status has null values\n",
    "assert offer_by_customer_labeled.offer_status.isnull().sum() == 0, 'There is a null value'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually check with 10 samples \n",
    "offer_by_customer_labeled.sample(10) # ok!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging subsetted data \n",
    "Now we merge the earlier subsetted `amount` & `reward` information for the ease pivoting operations. They are saved in `transaction_df` & `reward_df` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original dataframe \n",
    "df_merged = offer_by_customer_labeled.copy()\n",
    "\n",
    "# Reset index for merge \n",
    "df_merged = df_merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transaction_df` does not have `offer_id` so we merge based on the time that offers were completed. Also, offer_id, reward columns do not have any values other than NaN, and also event column is not unnecessary (it only has transaction as value) so drop them before the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transaction_df = transaction_df.drop(columns=['event', 'offer_id', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes\n",
    "df_merged = pd.merge(df_merged, transaction_df, how='left',\n",
    "                     left_on=['person', 'offer completed'], \n",
    "                     right_on=['person', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any record has been removed/added after the merge\n",
    "assert df_merged.shape[0] == 169940, \"There is a record lost/added from the merge\"\n",
    "assert df_merged[df_merged['offer completed'] != df_merged['time']]['offer completed'].sum() == 0\n",
    "\n",
    "# Then drop duplicated column due to difference in column name\n",
    "df_merged = df_merged.drop(columns='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no record lost or added after the merge which is good. However, note that the third row has a suspicious record where `offer_status` is incomplete but there still is an amount. Let's investigate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged[(df_merged['offer_status'] == 'incomplete') & (df_merged['amount'].notnull())].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very clear that transaction `amount` was added to incomplete offer when a customer actually made a transaction without viewing offer (so not influenced by the offer). \n",
    "\n",
    "Not action yet to take and moving on with merging `reward_df` for now. `reward_df` has unncessary columns thare are `event` (only offer completed has reward) and `amount` (no value) to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting reward_df \n",
    "reward_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unncessary columns\n",
    "reward_df = reward_df.drop(columns=['event', 'amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with reward_df \n",
    "df_merged = pd.merge(df_merged, reward_df, how='left',\n",
    "                     left_on=['person', 'offer_id', 'offer completed'], \n",
    "                     right_on=['person', 'offer_id', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any added/removed records after the merge\n",
    "assert df_merged.shape[0] == 169940, \"There is a record lost/added from the merge\"\n",
    "\n",
    "# Then, drop duplicated column due to difference in column name\n",
    "df_merged = df_merged.drop(columns='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"orangered\">Warning</font> Row 169936 has a suspicous record where offer completed time is earlier than offer viewed time. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Using the custom function defined earlier\n",
    "sample1 = transaction_details('ffff82501cea40309d5fdd7edcca4a07')\n",
    "sample1[sample1.time > 500][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LABELING ADJUSTMENT` Incorrect labeling\n",
    "\n",
    "It is noticeable that the customer viewed an offer after offer completed, and then the customer was not influenced by the offer and the offer is not considered complete.\n",
    "\n",
    "And the previous groupby operations assumed the dataframe was ordered by funnel but created a wide form pivot table with the minimum time (earliest record) so couldn't catch this exceptional case. \n",
    "\n",
    "There are over 3708 records that are incorrectly labeled, which need an additional adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_merged[df_merged['offer viewed'] > df_merged['offer completed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the incorrect labeling\n",
    "df_merged.loc[df_merged['offer viewed'] > df_merged['offer completed'], 'offer_status'] = 'incomplete'\n",
    "\n",
    "# Test the change\n",
    "assert (df_merged.loc[df_merged['offer viewed'] > df_merged['offer completed'], 'offer_status'] != 'incomplete').sum()  == 0, \"The adjustment failed...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LABELING ADJUSTMENT` Informational offer type\n",
    "In the beginning, we figured that there are two information offers and they only have two possible events: offer received and offer viewed. \n",
    "\n",
    "Therefore, we consider it completed when the two events are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df_merged dataframe\n",
    "temp = df_merged.copy()\n",
    "\n",
    "# Save all records with information offers and those not\n",
    "temp_informational = df_merged.loc[df_merged['offer_id'].isin(information_ids)]\n",
    "temp_transactional = df_merged.loc[~df_merged['offer_id'].isin(information_ids)]\n",
    "\n",
    "# Check if subsetting is done correctly\n",
    "assert temp_informational.shape[0] + temp_transactional.shape[0] == temp.shape[0], \"Wrong subsetting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Slicing data from temp_informational\n",
    "# that have values in offer received and offer viewed columns\n",
    "cond_received = temp_informational['offer received'].notnull()\n",
    "cond_viewed = temp_informational['offer viewed'].notnull()\n",
    "\n",
    "# Adjusting the incorrect labeling\n",
    "temp_informational.loc[cond_viewed & cond_viewed, 'offer_status'] = 'completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if changed correctly\n",
    "assert (temp_informational.loc[cond_viewed & cond_viewed]['offer_status'] == 'incomplete').sum() == 0, \"Failed adjustment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the change into the dataframe\n",
    "df_merged_adj = pd.concat([temp_informational, temp_transactional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If changed correctly save the dataframe\n",
    "df_merged_adj.to_csv('data/transcript_labeled.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"explore-part1\"></a>\n",
    "# `SECTION 3` Exploratory analysis : Part1\n",
    "\n",
    "Here we explore the individual dataset first. In the next section (Part2) we will continue the analysis with merged datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean dataset\n",
    "portfolio_v1 = pd.read_csv('data/portfolio_v1.csv')\n",
    "profile_v1 = pd.read_csv('data/profile_v1.csv', parse_dates=True)\n",
    "transcript_v1 = pd.read_csv('data/transcript_v1.csv')\n",
    "transcript_labeled = pd.read_csv('data/transcript_labeled.csv')\n",
    "\n",
    "del transcript_labeled['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Offer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_v1.groupby('offer_type').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "sns.heatmap(portfolio_v1.iloc[:, :5].groupby('offer_type').mean().T, \n",
    "            annot=True, fmt='.2f', cmap='Oranges', ax=ax1);\n",
    "sns.heatmap(portfolio_v1.iloc[:, 3:].groupby('offer_type').mean().T, \n",
    "            annot=True, fmt='.2f', cmap='Blues', ax=ax2);\n",
    "\n",
    "ax1.set(title ='How does each offer type look?');\n",
    "ax2.set(title ='Which channel does each offer use?');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`discount` offer requires the highest minium spends to get but has the longest duration, assuming customers feel the influence longer that the others. Regardless of offer type, `email` is always used when communicating the offer. `bogo` offer uses the widest number of channels. \n",
    "\n",
    "The analysis will be more meaningful when offer type is explored together with customer profile and transction data, which will be done later after mering the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b.  Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_v1.shape[0] # total 14825 unique demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks that 'became_member_on' is still object... \n",
    "# Convert to datetime object\n",
    "profile_v1['became_member_on'] = pd.to_datetime(profile_v1['became_member_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_color = sns.color_palette()[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12,8))\n",
    "\n",
    "fig1 = sns.countplot(x=profile_v1.gender, ax=axes[0,0], color=base_color)\n",
    "fig2 = sns.countplot(x=profile_v1.became_member_on.dt.year, ax=axes[0,1], color=base_color)\n",
    "fig3 = sns.histplot(x=profile_v1.age, binwidth=5, ax=axes[1,0])\n",
    "fig4 = sns.histplot(x=profile_v1.income, binwidth=5000, ax=axes[1,1])\n",
    "\n",
    "fig1.set(title='Count by gender', xlabel='', ylabel='')\n",
    "fig2.set(title='Count of new members by years', xlabel='', ylabel='')\n",
    "fig3.set(title='Age distribution of members', xlabel='', ylabel='')\n",
    "fig4.set(title='Income distribution of members\\n(unit: US dollars)', xlabel='', ylabel='')\n",
    "\n",
    "plt.tight_layout(pad=3.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of male, female and others\n",
    "profile_v1.gender.value_counts() / profile_v1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for age\n",
    "profile_v1.age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for income\n",
    "profile_v1.income.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gender:** based on this dataset, male customers accounts for 57% of total, more than female (or others). \n",
    "\n",
    "**Date:** The growth of new members has increased over time, but it may need better comparison against other metrics like the total visitor volume to validate the organic growth. Also, the data collection period is not clear so 2013 and 2018 data may be biased. \n",
    "\n",
    "**Age:** The minimum age is 18 which may be due to age restriction for members. With the median of 55, 50% of the members fall into interquartile range is between 42 and 66. There are some members above 100, which may be true or caused by survey error.\n",
    "\n",
    "**Income:** Median income is $64,000. The distribution is skewed to the right which seems natural for income distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. Offer completion\n",
    "Using the cleaned, pivoted dataframe saved in transcript_labeled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique customers\n",
    "transcript_labeled.person.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique offers should be 10\n",
    "transcript_labeled.offer_id.nunique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "fig1 = sns.countplot(x=transcript_labeled.offer_status, ax=ax1);\n",
    "\n",
    "# Proportion of offer completion over total sent\n",
    "offer_sent = transcript_labeled[transcript_labeled.offer_status != 'unsent'].offer_status.value_counts()\n",
    "fig2 = plt.bar(x=offer_sent.index, height=offer_sent.values / offer_sent.values.sum() * 100);\n",
    "\n",
    "ax1.set_title('Counts by Offer Status');\n",
    "ax2.set(title='Number of Offer Completion', ylabel='% total', xlabel='offer status');\n",
    "ax2.set_yticks(np.arange(0, 70+10, 10))\n",
    "ax2.set_yticklabels(np.arange(0, 70+10, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_labeled['offer_status'].value_counts() / transcript_labeled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_sent / offer_sent.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16,994 unique customers and 10 unique offers are represented in this dataset, which gives 169,940 total mix of possible offers. Around 63% offers are `unsent`, most likely because it was identified that offers do not fulfill the needs of specific customers or the opportunity was misinterpreted and therefore missed, which is the gap that we need to fill in this analysis. \n",
    "\n",
    "When considering those offers that were actually sent, about 52% turned out incomplete, which means offers did not influenced customers to purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset data for informational and transactional offers\n",
    "informational_ids = portfolio_v1.loc[portfolio_v1['offer_type'] == 'informational', 'id'].to_list()\n",
    "\n",
    "transcript_infos = transcript_labeled[transcript_labeled['offer_id'].isin(informational_ids)]\n",
    "transcript_trans = transcript_labeled[~transcript_labeled['offer_id'].isin(informational_ids)]\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
    "\n",
    "# Set order for plots\n",
    "status_order = ['unsent', 'incomplete', 'completed']\n",
    "\n",
    "fig1 = sns.countplot(x=transcript_infos.offer_status, ax=ax1, order=status_order);\n",
    "fig2 = sns.countplot(x=transcript_trans.offer_status, ax=ax2, order=status_order);\n",
    "\n",
    "ax1.set_title('Offer completion for information offers');\n",
    "ax2.set_title('Offer completion for transactional offers');\n",
    "ax2.set(ylabel='')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the split data, offer completion is relatively higher than incompletion for informational offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchased but incomplete offers\n",
    "Earlier it was identified that there are some offers incorrectly marked completed without customers actually viewing them. From the business perspective, this group of customers are not influenced by offers when purchasing products, it may not be a good idea to send offers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some conditions\n",
    "cond_purchased = transcript_labeled.amount.notnull()\n",
    "cond_incomplete = transcript_labeled.offer_status == 'incomplete'\n",
    "cond_viewed = transcript_labeled['offer viewed'].notnull()\n",
    "\n",
    "print(f'# offer incomplete (total): {transcript_labeled[cond_incomplete].shape[0]}')\n",
    "print(f'# offer incomplete but purchased: {transcript_labeled[cond_incomplete & cond_purchased].shape[0]}')\n",
    "print(f'# offer incomplete and viewed after purchase: {transcript_labeled[cond_incomplete & cond_purchased & cond_viewed].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purchase amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_labeled.amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two box plot graphs \n",
    "# fig1: incomplete but putchased, fig2: completed\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n",
    "fig1 = sns.boxplot(x=transcript_labeled[cond_incomplete].amount, ax=ax1);\n",
    "fig2 = sns.boxplot(x=transcript_labeled[~cond_incomplete].amount, ax=ax2);\n",
    "\n",
    "ax1.set(title='Summary statistics of purchased amount\\n(incomplete offers)', xlabel='amount ($)', ylabel='')\n",
    "ax2.set(title='Summary statistics of purchased amount\\n(completed offers)', xlabel='amount ($)', ylabel='')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purchase amount varies from 15 cents to 1015 dollars, while 75% fall below 23.39 dollars per receipt. Now let's see visualizations after removing outliers. Outliers here are difined as observations that are outside 1.5 x interquantile range below Q1 and above Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for statistics\n",
    "Q1_purchased = transcript_labeled.amount.describe()['25%']\n",
    "Q3_purchased = transcript_labeled.amount.describe()['75%']\n",
    "IQR_purchased = Q3_purchased - Q1_purchased\n",
    "outlier_lower = Q1_purchased - 1.5 * IQR_purchased\n",
    "outlier_upper = Q3_purchased + 1.5 * IQR_purchased\n",
    "\n",
    "# Two box plot graphs \n",
    "# fig1: incomplete but putchased, fig2: completed\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n",
    "fig1 = sns.boxplot(x=transcript_labeled[cond_incomplete].amount, ax=ax1);\n",
    "fig2 = sns.boxplot(x=transcript_labeled[~cond_incomplete].amount, ax=ax2);\n",
    "\n",
    "ax1.set(title='Summary statistics of purchased amount\\n(incomplete offers)',xlabel='', ylabel='')\n",
    "ax2.set(title='(completed offers)', xlabel='', ylabel='')\n",
    "\n",
    "ax1.set_xticks(np.arange(0, 100, 5))\n",
    "ax1.set_xticklabels([f'${str(n)}' for n in np.arange(0, 100, 5)])\n",
    "ax1.set_xlim(outlier_lower, outlier_upper+5)\n",
    "\n",
    "plt.tight_layout(pad=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those customers who purchased products without viewing offers have slightly higher spends when compared with those purchases influenced by others. It is self-explanatory as completed offers should have some discounts applied to them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"explore-part2\"></a>\n",
    "# `Section4` Exploratory analysis - Part2\n",
    "|\n",
    "In this section, we would like to go deeper into offer completion by offer type and demographics. In order to do this, we will merge the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unsent` offer status does not clearly show if customers completed offers or not, so we will subset those offers that customers actually received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4a. Completion by offer type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean dataset\n",
    "portfolio_v1 = pd.read_csv('data/portfolio_v1.csv', parse_dates=True)\n",
    "transcript_labeled = pd.read_csv('data/transcript_labeled.csv')\n",
    "del transcript_labeled['Unnamed: 0']\n",
    "\n",
    "\n",
    "# Create a function to prepare dataset for analysis\n",
    "def prepare_completion_by_offer(df1=transcript_labeled, df2=portfolio_v1):\n",
    "    ''' Function to prepare a dataframe ready for the analysis\n",
    "    of offer completion by offer type.\n",
    "    \n",
    "    INPUT (set by default): \n",
    "        df1: the transcript_labeled dataset \n",
    "        df2: the cleaned portfolio_v1 dataset\n",
    "        \n",
    "    OUTPUT: a new merged dataframe \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get all records that were sent (excluding unsent)\n",
    "    offer_sent = df1[df1['offer_status'] != 'unsent']\n",
    "    \n",
    "    # Create a dataframe that shows\n",
    "    # number of completed and incomplete offers by offer id\n",
    "    completion_by_offer = offer_sent.groupby(['offer_id', 'offer_status']).size().unstack()\n",
    "    \n",
    "    # Compute completion rate\n",
    "    completion_by_offer['completion_rate'] = completion_by_offer['completed'] / completion_by_offer.sum(axis=1).values\n",
    "    completion_by_offer['incompletion_rate'] = completion_by_offer['incomplete'] / completion_by_offer.sum(axis=1).values\n",
    "    \n",
    "    # Sort by completed rate\n",
    "    completion_by_offer.sort_values('completion_rate', ascending=False, inplace=True)\n",
    "    \n",
    "    # Merge the data sets\n",
    "    completion_by_offer_merged = pd.merge(completion_by_offer.reset_index(), df2, \n",
    "                                          left_on='offer_id', right_on='id', how='left')\n",
    "    \n",
    "    # Drop duplicated id column\n",
    "    completion_by_offer_merged.drop(columns='id', inplace=True)\n",
    "    \n",
    "    # Print the merged dataset\n",
    "    return completion_by_offer_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prepared dataset\n",
    "completion_by_offer = prepare_completion_by_offer()\n",
    "completion_by_offer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get more intuitive offer names with mix of values\n",
    "\n",
    "def get_offer_name(df, cols):\n",
    "    ''' Concatenate cell data from a dataframe to formulate a unique offer name\n",
    "    For example, if input cols are 'offer_type', 'reward', 'difficulty', 'duration'\n",
    "    and respective values are 'discount', 2, 10, 10, returning name will be disc021010\n",
    "    \n",
    "    INPUT: \n",
    "        df: a dataframe that contains information for offer names \n",
    "            i.e. 'offer_type', 'reward', 'difficulty', 'duration'\n",
    "        cols: columns that values will be extracted from\n",
    "        \n",
    "    OUTPUT: \n",
    "        a list of unique offer name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get the subset of data needed\n",
    "    df_subset = df[cols]\n",
    "    \n",
    "    # Instantiate a list of offer names\n",
    "    offer_names = list()\n",
    "    \n",
    "    # Iterate by row\n",
    "    for idx, values in df_subset.iterrows():\n",
    "        \n",
    "        # Instantiate name\n",
    "        name = ''\n",
    "        \n",
    "        for item in values:\n",
    "            # If item is string, get the first 4 letters\n",
    "            if type(item) == str:\n",
    "                item = item[:4]\n",
    "            \n",
    "            # If item is integer, check if the value < 10, \n",
    "            # then add '0' in front and transform it to string\n",
    "            if type(item) == int and item < 10:\n",
    "                item = '0' + str(item)\n",
    "            else:\n",
    "                item = str(item)\n",
    "            \n",
    "            # Concatenate item \n",
    "            name += item\n",
    "        \n",
    "        # Add completed name to a list of offer names\n",
    "        offer_names.append(name)\n",
    "        \n",
    "        \n",
    "    return offer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the offer names to the dataframe and set it as an index\n",
    "col_names = ['offer_type', 'reward', 'difficulty', 'duration']\n",
    "completion_by_offer['offer_name'] = get_offer_name(completion_by_offer, col_names)\n",
    "completion_by_offer.set_index('offer_name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_by_offer # good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "completion_by_offer[['completion_rate', 'incompletion_rate']].plot(kind='bar', stacked=True, figsize=(12,5));\n",
    "\n",
    "plt.title('Completion rate by offer type')\n",
    "plt.xlabel('offer type')\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel('completion rate (1.0 = 100%)')\n",
    "plt.legend(loc='upper center', ncol=4)\n",
    "plt.hlines(0.5, xmin=-10, xmax=100, linestyles='--', colors=sns.color_palette()[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informational offers have generally higher completion rate. Depsite slightly longer duration, completion rate of `info000003` is a lot higher than that of `info000004`. The difference possibly comes from type of distribution channel: `info000003` used social whereas `info info000004` used web. \n",
    "\n",
    "For transactional data, the completion rate is over 50% for `disc021010`, `disc030707`, `bogo050505`. \n",
    "\n",
    "`disc021010`, `disc030707` actually gives out less rewards with relatively higher difficulties, which is interesting. On ther other hand, they have relatively higher duration allowed for redemption and been distributed across all avilable channels - so the offers have been exposed to customers longer and wider.\n",
    "\n",
    "Note that there are two offers that are informational and they have completion rate of 0%, which originally had only two events: offer received and offer viewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe in case of any future use\n",
    "completion_by_offer.to_csv('data/completion_by_offer.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. Offer completion by demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clean dataset\n",
    "profile_v1 = pd.read_csv('data/profile_v1.csv', parse_dates=True)\n",
    "transcript_labeled = pd.read_csv('data/transcript_labeled.csv')\n",
    "del transcript_labeled['Unnamed: 0']\n",
    "\n",
    "\n",
    "# Create a function to prepare dataset for analysis\n",
    "def prepare_completion_by_demo(df1=transcript_labeled, df2=profile_v1):\n",
    "    ''' Function to prepare a dataframe ready for the analysis\n",
    "    of offer completion by demographics.\n",
    "    \n",
    "    INPUT (set by default): \n",
    "        df1: the transcript_labeled dataset \n",
    "        df2: the cleaned profile_v1 dataset\n",
    "        \n",
    "    OUTPUT: a new merged dataframe \n",
    "    '''\n",
    "    \n",
    "    # Get all records that were sent (excluding unsent)\n",
    "    offer_sent = df1[df1['offer_status'] != 'unsent']\n",
    "    \n",
    "    # Merge with profile_v1 dataset\n",
    "    completion_by_demo = pd.merge(offer_sent, df2, \n",
    "                                  left_on='person', right_on='id', how='left')\n",
    "\n",
    "    # Drop the duplicated id column\n",
    "    completion_by_demo = completion_by_demo.drop(columns='id')\n",
    "\n",
    "    \n",
    "    return completion_by_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_by_demo = prepare_completion_by_demo()\n",
    "completion_by_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe in case of any future use\n",
    "completion_by_demo.to_csv('data/completion_by_demo.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion by gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below heatmap visualization was inspired by https://moonbooks.org/Articles/How-to-add-text-units--etc-in-a-heatmap-cell-annotations-using-seaborn-in-python-/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_by_gender = completion_by_demo.groupby(['offer_status', 'gender']).size().unstack()\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "fig = sns.heatmap(completion_by_gender / completion_by_gender.sum(axis=0) * 100, \n",
    "                  annot=True, fmt='.2f', cmap=cmap)\n",
    "\n",
    "# Fine tune annotation\n",
    "for t in fig.texts: \n",
    "    t.set_text(t.get_text() + \" %\")\n",
    "    \n",
    "plt.title('Offer completion by gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Female customers generally have higher completion rate than male. Male customers have less offers completed than incomplete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each record with age group\n",
    "bin_edges = np.arange(10, 100+10, 10)\n",
    "bin_label = [str(n)+ 's' for n in bin_edges[:-1]]\n",
    "completion_by_demo['age_group'] = pd.cut(completion_by_demo.age, bins=bin_edges, labels=bin_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_by_age = completion_by_demo.groupby(['offer_status', 'age_group']).size().unstack()\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "plt.figure(figsize=(12,5))\n",
    "fig = sns.heatmap(completion_by_age / completion_by_age.sum(axis=0) * 100, \n",
    "                  annot=True, fmt='.2f', cmap=cmap)\n",
    "\n",
    "# Fine tune annotation\n",
    "for t in fig.texts: \n",
    "    t.set_text(t.get_text() + \" %\")\n",
    "    \n",
    "plt.title('Offer completion by gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion rate is low (lower than incomplete) for customers below 30s whereas those above 40s have completion rate higher than 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion by income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each record with income group\n",
    "bin_edges = np.arange(completion_by_demo.income.min(), \n",
    "                       completion_by_demo.income.max() + 10000, 10000)\n",
    "bin_label = ['$' + str(int(n))[:-3] + 'k' for n in bin_edges[:-1]]\n",
    "completion_by_demo['income_group'] = pd.cut(completion_by_demo.income, bins=bin_edges, labels=bin_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_by_income = completion_by_demo.groupby(['offer_status', 'income_group']).size().unstack()\n",
    "\n",
    "cmap = sns.cubehelix_palette(as_cmap=True)\n",
    "plt.figure(figsize=(12,5))\n",
    "fig = sns.heatmap(completion_by_income / completion_by_income.sum(axis=0) * 100, \n",
    "                  annot=True, fmt='.2f', cmap=cmap)\n",
    "\n",
    "# Fine tune annotation\n",
    "for t in fig.texts: \n",
    "    t.set_text(t.get_text() + \" %\")\n",
    "    \n",
    "plt.title('Offer completion by gender')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completion rate is lower for customers with income less than \\\\$50k. The higest completion rate is observed in the income group between \\\\$80k and \\\\$100k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"feature\"></a>\n",
    "## `SECTION5` Feature engineering\n",
    "Now merge the cleaned dataset to prepare for a classifer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "portfolio_cleaned = pd.read_csv('data/portfolio_v1.csv')\n",
    "profile_cleaned = pd.read_csv('data/profile_v1.csv')\n",
    "transcript_labeled = pd.read_csv('data/transcript_labeled.csv')\n",
    "\n",
    "del transcript_labeled['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Merging the dataset - 1\n",
    "the cleaned transcript and profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>offer received</th>\n",
       "      <th>offer viewed</th>\n",
       "      <th>offer completed</th>\n",
       "      <th>offer_status</th>\n",
       "      <th>amount</th>\n",
       "      <th>reward</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>id</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>336.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>168.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00116118485d4dfda04fdbaba9a87b5c</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00116118485d4dfda04fdbaba9a87b5c</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unsent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>completed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>57000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person                          offer_id  \\\n",
       "0  0009655768c64bdeb2e877511632db8f  3f207df678b143eea3cee63160fa8bed   \n",
       "1  0009655768c64bdeb2e877511632db8f  5a8bc65990b245e5a138643cd4eb9837   \n",
       "2  00116118485d4dfda04fdbaba9a87b5c  3f207df678b143eea3cee63160fa8bed   \n",
       "3  00116118485d4dfda04fdbaba9a87b5c  5a8bc65990b245e5a138643cd4eb9837   \n",
       "4  0011e0d4e6b944f998e987f904e8c1e5  3f207df678b143eea3cee63160fa8bed   \n",
       "\n",
       "   offer received  offer viewed  offer completed offer_status  amount  reward  \\\n",
       "0           336.0         372.0              NaN    completed     NaN     NaN   \n",
       "1           168.0         192.0              NaN    completed     NaN     NaN   \n",
       "2             NaN           NaN              NaN       unsent     NaN     NaN   \n",
       "3             NaN           NaN              NaN       unsent     NaN     NaN   \n",
       "4             0.0           6.0              NaN    completed     NaN     NaN   \n",
       "\n",
       "  gender   age                                id became_member_on   income  \n",
       "0      M  33.0  0009655768c64bdeb2e877511632db8f       2017-04-21  72000.0  \n",
       "1      M  33.0  0009655768c64bdeb2e877511632db8f       2017-04-21  72000.0  \n",
       "2    NaN   NaN                               NaN              NaN      NaN  \n",
       "3    NaN   NaN                               NaN              NaN      NaN  \n",
       "4      O  40.0  0011e0d4e6b944f998e987f904e8c1e5       2018-01-09  57000.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_first = pd.merge(transcript_labeled, profile_cleaned, left_on='person', right_on='id', how='left')\n",
    "df_merged_first.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 169940 entries, 0 to 169939\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   person            169940 non-null  object \n",
      " 1   offer_id          169940 non-null  object \n",
      " 2   offer received    63288 non-null   float64\n",
      " 3   offer viewed      49135 non-null   float64\n",
      " 4   offer completed   28996 non-null   float64\n",
      " 5   offer_status      169940 non-null  object \n",
      " 6   amount            28996 non-null   float64\n",
      " 7   reward            28996 non-null   float64\n",
      " 8   gender            148200 non-null  object \n",
      " 9   age               148200 non-null  float64\n",
      " 10  id                148200 non-null  object \n",
      " 11  became_member_on  148200 non-null  object \n",
      " 12  income            148200 non-null  float64\n",
      "dtypes: float64(7), object(6)\n",
      "memory usage: 18.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_merged_first.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "\n",
    "There are null values in gender, age, became_member_on and income columns that are resulted from the removed records for `profile_clean` (those missing gender and income data with age of 118), which can be dropped agin. \n",
    "\n",
    "Also, there are columns that are not informational, which need to be dropped\n",
    "- id column,\n",
    "- offer received, offer viewed and offer completed\n",
    "\n",
    "Lastly, there are many missing values in amount, reward columns indicating no transactions completed. We will the values with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id', 'offer received', 'offer viewed', 'offer completed']\n",
    "df_merged_first.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill amount, reward columns with 0\n",
    "df_merged_first[['amount', 'reward']] = df_merged_first[['amount', 'reward']].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop entries with no gender, age and income values\n",
    "df_merged_first.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change for null values\n",
    "assert df_merged_first.isnull().sum().sum() == 0, \"There still is a missing value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Merging the dataset - 2\n",
    "df_merged_first and portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>offer_status</th>\n",
       "      <th>amount</th>\n",
       "      <th>reward_x</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "      <th>reward_y</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>id</th>\n",
       "      <th>web</th>\n",
       "      <th>email</th>\n",
       "      <th>social</th>\n",
       "      <th>mobile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>informational</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>informational</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0020c2b971eb4e9188eac86d93036a77</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>unsent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person                          offer_id  \\\n",
       "0  0009655768c64bdeb2e877511632db8f  3f207df678b143eea3cee63160fa8bed   \n",
       "1  0009655768c64bdeb2e877511632db8f  5a8bc65990b245e5a138643cd4eb9837   \n",
       "2  0011e0d4e6b944f998e987f904e8c1e5  3f207df678b143eea3cee63160fa8bed   \n",
       "3  0011e0d4e6b944f998e987f904e8c1e5  5a8bc65990b245e5a138643cd4eb9837   \n",
       "4  0020c2b971eb4e9188eac86d93036a77  3f207df678b143eea3cee63160fa8bed   \n",
       "\n",
       "  offer_status  amount  reward_x gender   age became_member_on   income  \\\n",
       "0    completed     0.0       0.0      M  33.0       2017-04-21  72000.0   \n",
       "1    completed     0.0       0.0      M  33.0       2017-04-21  72000.0   \n",
       "2    completed     0.0       0.0      O  40.0       2018-01-09  57000.0   \n",
       "3    completed     0.0       0.0      O  40.0       2018-01-09  57000.0   \n",
       "4       unsent     0.0       0.0      F  59.0       2016-03-04  90000.0   \n",
       "\n",
       "   reward_y  difficulty  duration     offer_type  \\\n",
       "0         0           0         4  informational   \n",
       "1         0           0         3  informational   \n",
       "2         0           0         4  informational   \n",
       "3         0           0         3  informational   \n",
       "4         0           0         4  informational   \n",
       "\n",
       "                                 id  web  email  social  mobile  \n",
       "0  3f207df678b143eea3cee63160fa8bed    1      1       0       1  \n",
       "1  5a8bc65990b245e5a138643cd4eb9837    0      1       1       1  \n",
       "2  3f207df678b143eea3cee63160fa8bed    1      1       0       1  \n",
       "3  5a8bc65990b245e5a138643cd4eb9837    0      1       1       1  \n",
       "4  3f207df678b143eea3cee63160fa8bed    1      1       0       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged = pd.merge(df_merged_first, portfolio_cleaned, left_on='offer_id', right_on='id', how='left')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 148200 entries, 0 to 148199\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   person            148200 non-null  object \n",
      " 1   offer_id          148200 non-null  object \n",
      " 2   offer_status      148200 non-null  object \n",
      " 3   amount            148200 non-null  float64\n",
      " 4   reward_x          148200 non-null  float64\n",
      " 5   gender            148200 non-null  object \n",
      " 6   age               148200 non-null  float64\n",
      " 7   became_member_on  148200 non-null  object \n",
      " 8   income            148200 non-null  float64\n",
      " 9   reward_y          148200 non-null  int64  \n",
      " 10  difficulty        148200 non-null  int64  \n",
      " 11  duration          148200 non-null  int64  \n",
      " 12  offer_type        148200 non-null  object \n",
      " 13  id                148200 non-null  object \n",
      " 14  web               148200 non-null  int64  \n",
      " 15  email             148200 non-null  int64  \n",
      " 16  social            148200 non-null  int64  \n",
      " 17  mobile            148200 non-null  int64  \n",
      "dtypes: float64(4), int64(7), object(7)\n",
      "memory usage: 21.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There shouldn't be any missing values\n",
    "assert df_merged.isnull().sum().sum() == 0, \"There is a missing value in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentary:\n",
    "- There is no null values.  \n",
    "- Drop the duplicated columns: `reward` and `id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(columns=['reward_x', 'id'], inplace=True)\n",
    "df_merged.rename(columns={'reward_y':'reward'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the merged dataframe \n",
    "df_merged.to_csv('data/starbucks_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Feature engineering\n",
    "\n",
    "We are going to make a model that predicts if offer will be completed or incomplete. \n",
    "Also, we will see if there is any possibility of offer completion for those offers unsent. \n",
    "\n",
    "In order to do that, we will train a classifier model with the subset dataset from `df_merged` where `offer_status` is either completed or incomplete - so **unsent** label will be set aside. In addition, non-numeric features which needs some encoding and features should be standardized to ensure similar sacling applied.  \n",
    "\n",
    "#### Here is a list of preprocessing needed:\n",
    "\n",
    "**target variable** : `offer_status`\n",
    "- Create the subset dataframe excluding **unsent** offer status\n",
    "- Then **offer_status** will be transformed into binary values (1: completed, 2: incomplete)\n",
    "\n",
    "\n",
    "**features**\n",
    "- drop **person** column\n",
    "- drop **offer_id**: **offer_id** & **offer_type** somehow deliver similar information \n",
    "- convert **gender** & **offer_type** features into dummy variables \n",
    "- destructure **became_member_on** into joined_year and joined_month columns (int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>offer_status</th>\n",
       "      <th>amount</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>became_member_on</th>\n",
       "      <th>income</th>\n",
       "      <th>reward</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>duration</th>\n",
       "      <th>offer_type</th>\n",
       "      <th>web</th>\n",
       "      <th>email</th>\n",
       "      <th>social</th>\n",
       "      <th>mobile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009655768c64bdeb2e877511632db8f</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011e0d4e6b944f998e987f904e8c1e5</td>\n",
       "      <td>5a8bc65990b245e5a138643cd4eb9837</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2018-01-09</td>\n",
       "      <td>57000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>informational</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0020c2b971eb4e9188eac86d93036a77</td>\n",
       "      <td>3f207df678b143eea3cee63160fa8bed</td>\n",
       "      <td>unsent</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>informational</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             person                          offer_id  \\\n",
       "0  0009655768c64bdeb2e877511632db8f  3f207df678b143eea3cee63160fa8bed   \n",
       "1  0009655768c64bdeb2e877511632db8f  5a8bc65990b245e5a138643cd4eb9837   \n",
       "2  0011e0d4e6b944f998e987f904e8c1e5  3f207df678b143eea3cee63160fa8bed   \n",
       "3  0011e0d4e6b944f998e987f904e8c1e5  5a8bc65990b245e5a138643cd4eb9837   \n",
       "4  0020c2b971eb4e9188eac86d93036a77  3f207df678b143eea3cee63160fa8bed   \n",
       "\n",
       "  offer_status  amount gender   age became_member_on   income  reward  \\\n",
       "0    completed     0.0      M  33.0       2017-04-21  72000.0       0   \n",
       "1    completed     0.0      M  33.0       2017-04-21  72000.0       0   \n",
       "2    completed     0.0      O  40.0       2018-01-09  57000.0       0   \n",
       "3    completed     0.0      O  40.0       2018-01-09  57000.0       0   \n",
       "4       unsent     0.0      F  59.0       2016-03-04  90000.0       0   \n",
       "\n",
       "   difficulty  duration     offer_type  web  email  social  mobile  \n",
       "0           0         4  informational    1      1       0       1  \n",
       "1           0         3  informational    0      1       1       1  \n",
       "2           0         4  informational    1      1       0       1  \n",
       "3           0         3  informational    0      1       1       1  \n",
       "4           0         4  informational    1      1       0       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the merged dataset\n",
    "df_merged = pd.read_csv('data/starbucks_merged.csv')\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(df, cols):\n",
    "    ''' Convert categorical variables into dummary variables\n",
    "    \n",
    "    INPUT: \n",
    "        df: a dataframe that contains a column of categorical data  \n",
    "        cols: the columns that need conversion\n",
    "    OUTPUT: a converted dataframe in binary values\n",
    "    '''\n",
    "    \n",
    "    return pd.get_dummies(df[cols], dtype=int)\n",
    "\n",
    "\n",
    "def destucture_date(df, col, fmt='%Y-%m-%d'):\n",
    "    ''' Convert a date column into date object \n",
    "    following the input format, and destructure it into\n",
    "    year, month and day\n",
    "    \n",
    "    INPUT:\n",
    "        df: a dataframe that contains a column of date information  \n",
    "        col: the column with data values\n",
    "        fmt: default as %Y-%m-%d, date format of the input col\n",
    "    \n",
    "    OUTPUT: a series of year, month and day\n",
    "    '''\n",
    "    \n",
    "    # Trnasform the input column into datetime object \n",
    "    date_obj = pd.to_datetime(df[col], format=fmt)\n",
    "\n",
    "    # Destucture date_obj into a series of year, month and day\n",
    "    year = date_obj.dt.year\n",
    "    month = date_obj.dt.month\n",
    "    day = date_obj.dt.day\n",
    "    \n",
    "    \n",
    "    return year, month, day\n",
    "    \n",
    "\n",
    "def prepare_features(df=df_merged):\n",
    "    ''' Get the merged dataframe that needs feature engineering\n",
    "    and return a target variable and features ready for classification\n",
    "    as well as a dataframe that contains unsent offer data\n",
    "    \n",
    "    INPUT: a dataframe that is ready for feature engineering\n",
    "    OUTPUT: a target variable, a set of features and a dataframe with unsent\n",
    "    offer data\n",
    "    \n",
    "    '''\n",
    "    # Get the subset of offers sent and unsent\n",
    "    offer_sent = df.query('offer_status != \"unsent\"')\n",
    "    offer_unsent = df.query('offer_status == \"unsent\"')\n",
    "        \n",
    "    #### Now we perform feature engineering on 'offer_sent'  \n",
    "        \n",
    "    # Get target variable: 'offer_status' & features\n",
    "    target = offer_sent['offer_status'].map({'completed':1, 'incomplete':0})\n",
    "    \n",
    "    # Get features by dropping 'offer_status' (target variable )\n",
    "    # Also drop unnessary 'person' and 'offer_id' features\n",
    "    features = offer_sent.drop(columns=['offer_status', 'person', 'offer_id'])\n",
    "    \n",
    "    # Convert gender and offer_type into dummy variables\n",
    "    # then join into the features \n",
    "    # then drop 'gender', 'offer_type' columns\n",
    "    dummies = get_dummies(features, ['gender', 'offer_type'])\n",
    "    features = features.join(dummies).drop(columns=['gender', 'offer_type'])\n",
    "\n",
    "    # Destructure became_member_on column into year, month and day\n",
    "    # then add series of year and month into features (exclude day: too granular)\n",
    "    # then drop 'became_member_on' column\n",
    "    year, month, day  = destucture_date(features, 'became_member_on')\n",
    "    features['joined_year'] = year\n",
    "    features['joined_month'] = month\n",
    "    \n",
    "    features = features.drop(columns='became_member_on')\n",
    "    \n",
    "    \n",
    "    return features, target, offer_unsent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features, target, offer_unsent = prepare_features(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering has been completed!\n"
     ]
    }
   ],
   "source": [
    "# Confirm the result\n",
    "assert target.nunique() == 2\n",
    "assert len(target) == df_merged.query('offer_status!=\"unsent\"').shape[0]\n",
    "assert len(features) == df_merged.query('offer_status!=\"unsent\"').shape[0]\n",
    "assert offer_unsent.offer_status.unique() == 'unsent'\n",
    "print('Feature engineering has been completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55222 entries, 0 to 148199\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   amount                    55222 non-null  float64\n",
      " 1   age                       55222 non-null  float64\n",
      " 2   income                    55222 non-null  float64\n",
      " 3   reward                    55222 non-null  int64  \n",
      " 4   difficulty                55222 non-null  int64  \n",
      " 5   duration                  55222 non-null  int64  \n",
      " 6   web                       55222 non-null  int64  \n",
      " 7   email                     55222 non-null  int64  \n",
      " 8   social                    55222 non-null  int64  \n",
      " 9   mobile                    55222 non-null  int64  \n",
      " 10  gender_F                  55222 non-null  int64  \n",
      " 11  gender_M                  55222 non-null  int64  \n",
      " 12  gender_O                  55222 non-null  int64  \n",
      " 13  offer_type_bogo           55222 non-null  int64  \n",
      " 14  offer_type_discount       55222 non-null  int64  \n",
      " 15  offer_type_informational  55222 non-null  int64  \n",
      " 16  joined_year               55222 non-null  int64  \n",
      " 17  joined_month              55222 non-null  int64  \n",
      "dtypes: float64(3), int64(15)\n",
      "memory usage: 10.5 MB\n"
     ]
    }
   ],
   "source": [
    "# all features are now numeric with no missing values\n",
    "features.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "# `SECTION6` Modeling\n",
    "\n",
    "We would like to build a classifier model to predict a customer with a particular offer type will make a purchase of a product influenced by the offer (complete the offer). \n",
    "\n",
    "We have two classes either **completed** or **incomplete** in our target varaible `offer_status` and would like to have the number of these two classes balanced (similar). This will give a good reference when choosing the split method for training and test sets. If the target variable is balanced, we will be able to use random split, otherwise we may need to consider using alternative method such as stratified split.\n",
    "\n",
    "To illustrate, let's take an example of a dataset of 1000 records where the target variable is not balanced (say class A has 900 and class B has 100 records). If random splitting is adopted withotu considering the balance, we may end up having train set with a majority of class A labels and the model will be biased. So we wouldl like to avoid this by checking **invariant metric** as below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Get variables ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variables for classification\n",
    "features, target, offer_unsent = prepare_features(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Splitting into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check invariant metric for the target variable\n",
    "\n",
    "def check_invariant_metric(series, alpha=0.05):\n",
    "    ''' Run simulations to see if the target variable has balanced \n",
    "    by testing the number of each class group is similar.\n",
    "    \n",
    "    Calculated p-value will be compared to the alpha value to see\n",
    "    if there is statistical sigificance enough the reject the null.\n",
    "    \n",
    "    The higher p-value, it is harder to reject the null which then\n",
    "    means target variable is balanced and good to go.\n",
    "    \n",
    "    Return True if successful False if not.\n",
    "\n",
    "    INPUT: \n",
    "        series: a Pandas series of target variable\n",
    "        alpha: level of significance that the calculated p-value\n",
    "            will be compared to for the final decision, default at 0.05\n",
    "\n",
    "    OUTPUT: boolean\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Set parameters under null\n",
    "    n_obs = target.shape[0]\n",
    "    p = 0.5\n",
    "    n_trials = 20000\n",
    "    \n",
    "    samples = np.random.binomial(n_obs, p, n_trials)\n",
    "    \n",
    "    # Get actual count - the number of 1(completed)\n",
    "    n_control = target.value_counts().values[0] \n",
    "    \n",
    "\n",
    "    # Calculate p-value for the 2-sided test\n",
    "    p_value = np.logical_or(samples <= n_control, \n",
    "                            samples >= (n_obs - n_control)).mean()\n",
    "       \n",
    "    # Decision rule: \n",
    "    if p_value < alpha:\n",
    "        # Reject the null: target variable is not balanced\n",
    "        return False\n",
    "    else:\n",
    "        # Fail to reject the null: target variable is balanced\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_train_test_sets(X, y):\n",
    "    ''' Split data into train and test sets\n",
    "    if target variable is balanced, continue with random split\n",
    "    or return None with error message\n",
    "    \n",
    "    INPUT:\n",
    "        X: features \n",
    "        y: taret variable\n",
    "        \n",
    "    OUTPUT:\n",
    "        X_train, X_test, y_train, y_test\n",
    "        or None (when the dataset is not balanced)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # True if the target variable is balanced\n",
    "    if check_invariant_metric(y):\n",
    "        return train_test_split(X, y)\n",
    "\n",
    "    else:\n",
    "        print('The dataset is not balanced',\n",
    "              'check for alternative split method')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random split the dataset - splitting should work\n",
    "X_train, X_test, y_train, y_test = split_train_test_sets(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Train logistic regression model (`model1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model01 = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model01.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_predict(model, X_test):\n",
    "    ''' Run predictions for a fitted classifer model\n",
    "    \n",
    "    INPUT:\n",
    "        model: a fitted classifer model\n",
    "        X_test: test features used for prediction\n",
    "        \n",
    "    OUTPUT:\n",
    "        y_pred: the predicted target variable\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "\n",
    "def print_classification_scores(y_test, y_pred):\n",
    "    ''' Show test scores for classification as one go\n",
    "    by combining accuracy score and classification report.\n",
    "    \n",
    "    INPUT: \n",
    "        model: the classifier model that fit training sets\n",
    "        y_test: a true y test values\n",
    "        y_pred: a predicted y values\n",
    "        \n",
    "    OUTPUT:\n",
    "        None\n",
    "        \n",
    "    '''\n",
    "       \n",
    "    # Get test scores\n",
    "    print(f'Accuracy score: {accuracy_score(y_test, y_pred) * 100:.2f}%')\n",
    "    print(f'Classfication report:\\n')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred01 = clf_predict(model01, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 73.86%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72      6811\n",
      "           1       0.72      0.79      0.75      6995\n",
      "\n",
      "    accuracy                           0.74     13806\n",
      "   macro avg       0.74      0.74      0.74     13806\n",
      "weighted avg       0.74      0.74      0.74     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_classification_scores(y_test, y_pred01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not extremely bad score for all metrics for the first model. Let's see see if standardization can improve the result in the pipeline, and then fit a model with different parameters by using GridSearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf01', LogisticRegression())]),\n",
       "             param_grid={'clf01__C': [0.1, 0.5, 1.0],\n",
       "                         'clf01__solver': ('lbfgs', 'sag')},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline01 = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf01', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Set different parameters for grid search\n",
    "params01 = {\n",
    "    'clf01__C': [0.1, 0.5, 1.0],\n",
    "    'clf01__solver': ('lbfgs', 'sag')\n",
    "}\n",
    "\n",
    "# Build a pipeline with grid search\n",
    "model01_search = GridSearchCV(pipeline01, params01, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "model01_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 76.71%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76      6811\n",
      "           1       0.76      0.79      0.78      6995\n",
      "\n",
      "    accuracy                           0.77     13806\n",
      "   macro avg       0.77      0.77      0.77     13806\n",
      "weighted avg       0.77      0.77      0.77     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "y_pred01_search = clf_predict(model01_search.best_estimator_, X_test)\n",
    "\n",
    "# Print classification scores\n",
    "print_classification_scores(y_test, y_pred01_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score went up by 3% with the best parameters searched. f1-score also improved!\n",
    "Let's see if the model can produce better result with other classifier methods like decision trees or random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the models folder\n",
    "with open(\"models/logreg.pkl\", 'wb') as file:\n",
    "    pickle.dump(model01_search.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "# with open(\"models/logreg.pkl\", 'rb') as file:\n",
    "#     model01_search = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Train Decision Tree classifier (`model2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model02 = DecisionTreeClassifier()\n",
    "model02.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred02 = clf_predict(model02, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 76.39%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.76      6811\n",
      "           1       0.77      0.76      0.76      6995\n",
      "\n",
      "    accuracy                           0.76     13806\n",
      "   macro avg       0.76      0.76      0.76     13806\n",
      "weighted avg       0.76      0.76      0.76     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any parameter seeting accuracy score and f1-score scores are close the best estimater of Logistic Regerssion. Let's do parameter setting and se if it can improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   30.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf02', DecisionTreeClassifier())]),\n",
       "             param_grid={'clf02__criterion': ('gini', 'entropy'),\n",
       "                         'clf02__min_samples_leaf': [1, 5, 10, 20, 50, 100],\n",
       "                         'clf02__min_samples_split': [2, 10, 20, 50, 100]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline02 = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf02', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Set different parameters for grid search\n",
    "params02 = {\n",
    "    'clf02__criterion': ('gini', 'entropy'),\n",
    "    'clf02__min_samples_split': [2, 10, 20, 50, 100],\n",
    "    'clf02__min_samples_leaf': [1, 5, 10, 20, 50, 100]\n",
    "}\n",
    "\n",
    "# Build a pipeline with grid search\n",
    "model02_search = GridSearchCV(pipeline02, params02, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "model02_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('clf02', DecisionTreeClassifier(min_samples_leaf=50))])\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "print(model02_search.best_estimator_)\n",
    "y_pred02_search = clf_predict(model02_search.best_estimator_, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 82.44%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81      6811\n",
      "           1       0.79      0.89      0.84      6995\n",
      "\n",
      "    accuracy                           0.82     13806\n",
      "   macro avg       0.83      0.82      0.82     13806\n",
      "weighted avg       0.83      0.82      0.82     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred02_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the test score a lot more improved. After running grid search several times, it looks that the test score gets better with higher `min_samples_split` (**try change to verbose=3 in GridSearchCV**). So let's test it with `min_samples_split` parameter only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] clf02__min_samples_leaf=100 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=100, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=100 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=100, score=0.824, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=100 .....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ......... clf02__min_samples_leaf=100, score=0.825, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=100 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=100, score=0.827, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=100 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=100, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=200 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=200, score=0.831, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=200 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=200, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=200 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=200, score=0.831, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=200 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=200, score=0.830, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=200 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=200, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=300 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=300, score=0.830, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=300 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=300, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=300 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=300, score=0.831, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=300 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=300, score=0.831, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=300 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=300, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=400 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=400, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=400 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=400, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=400 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=400, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=400 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=400, score=0.831, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=400 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=400, score=0.825, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=500 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=500, score=0.827, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=500 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=500, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=500 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=500, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=500 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=500, score=0.828, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=500 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=500, score=0.822, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=600 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=600, score=0.829, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=600 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=600, score=0.817, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=600 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=600, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=600 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=600, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=600 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=600, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=700 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=700, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=700 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=700, score=0.817, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=700 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=700, score=0.825, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=700 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=700, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=700 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=700, score=0.821, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=800 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=800, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=800 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=800, score=0.818, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=800 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=800, score=0.824, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=800 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=800, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=800 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=800, score=0.820, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=900 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=900, score=0.826, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=900 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=900, score=0.814, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=900 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=900, score=0.822, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=900 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=900, score=0.825, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=900 .....................................\n",
      "[CV] ......... clf02__min_samples_leaf=900, score=0.817, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=1000 ....................................\n",
      "[CV] ........ clf02__min_samples_leaf=1000, score=0.825, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=1000 ....................................\n",
      "[CV] ........ clf02__min_samples_leaf=1000, score=0.814, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=1000 ....................................\n",
      "[CV] ........ clf02__min_samples_leaf=1000, score=0.820, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=1000 ....................................\n",
      "[CV] ........ clf02__min_samples_leaf=1000, score=0.823, total=   0.1s\n",
      "[CV] clf02__min_samples_leaf=1000 ....................................\n",
      "[CV] ........ clf02__min_samples_leaf=1000, score=0.816, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf02', DecisionTreeClassifier())]),\n",
       "             param_grid={'clf02__min_samples_leaf': array([ 100,  200,  300,  400,  500,  600,  700,  800,  900, 1000])},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline02b = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf02', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Set different parameters for grid search\n",
    "params02b = {\n",
    "    'clf02__min_samples_leaf': np.arange(0,1000+100,100)[1:]\n",
    "}\n",
    "\n",
    "# Build a pipeline with grid search\n",
    "model02_search_b = GridSearchCV(pipeline02b, params02b, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "model02_search_b.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('clf02', DecisionTreeClassifier(min_samples_leaf=200))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model02_search_b.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 83.09%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82      6811\n",
      "           1       0.80      0.89      0.84      6995\n",
      "\n",
      "    accuracy                           0.83     13806\n",
      "   macro avg       0.84      0.83      0.83     13806\n",
      "weighted avg       0.84      0.83      0.83     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred02_search_b = clf_predict(model02_search_b.best_estimator_, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred02_search_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator turned to be with the parameter of `min_samples_leaf=200` which is slightly better than with the previous one with `in_samples_leaf=100.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the models folder\n",
    "with open(\"models/dtree.pkl\", 'wb') as file:\n",
    "    pickle.dump(model02_search_b.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "# with open(\"models/dtree.pkl\", 'rb') as file:\n",
    "#     model02_search = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5e. Train Suppor Vector Machine classifier (`model3`)\n",
    "\n",
    "After a trial, it was found that SVC is extremely slow as it trains with non-linear kernel (stack overflow reference [here](https://stackoverflow.com/questions/40077432/why-is-scikit-learn-svm-svc-extremely-slow)). So let's try with LinearSVC where no kernels are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model03 = LinearSVC(max_iter=10000) # fit linear svc first\n",
    "model03.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_iter set to 10000 as the default threw a convergence error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 49.59%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66      6811\n",
      "           1       0.75      0.01      0.01      6995\n",
      "\n",
      "    accuracy                           0.50     13806\n",
      "   macro avg       0.62      0.50      0.34     13806\n",
      "weighted avg       0.63      0.50      0.33     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred03 = clf_predict(model03, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm...the model using Linear SVC did not improve the test result. Assuming that it is because SVM is sensitive to different scaling, let's try to build a pipeline using standard scaler [Stack overflow reference](https://stackoverflow.com/questions/39001936/techniques-to-improve-the-accuracy-of-svm-classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('clf03', LinearSVC(max_iter=10000))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline03 = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf03', LinearSVC(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Fit the data to pipeline03\n",
    "pipeline03.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 71.83%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72      6811\n",
      "           1       0.72      0.72      0.72      6995\n",
      "\n",
      "    accuracy                           0.72     13806\n",
      "   macro avg       0.72      0.72      0.72     13806\n",
      "weighted avg       0.72      0.72      0.72     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred03b = clf_predict(pipeline03, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred03b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is improved with scaler but as much as the previous ones using other classifiers. Let's see if parameter setting can improve the model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV] clf03__C=0.1, clf03__max_iter=5000 ..............................\n",
      "[CV] ............... clf03__C=0.1, clf03__max_iter=5000, total=  12.3s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.1, clf03__max_iter=5000, total=  12.6s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=5000 ..............................\n",
      "[CV] ............... clf03__C=0.1, clf03__max_iter=5000, total=   9.9s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=5000 ..............................\n",
      "[CV] ............... clf03__C=0.1, clf03__max_iter=5000, total=  11.6s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=5000 ..............................\n",
      "[CV] ............... clf03__C=0.1, clf03__max_iter=5000, total=  11.8s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=10000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=10000, total=  11.0s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=10000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=10000, total=  11.9s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=10000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=10000, total=   9.7s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=10000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=10000, total=  13.1s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=10000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=10000, total=  12.9s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=20000, total=  12.8s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=20000, total=  12.7s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=20000, total=  11.0s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=20000, total=  12.0s\n",
      "[CV] clf03__C=0.1, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.1, clf03__max_iter=20000, total=  12.5s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.5, clf03__max_iter=5000, total=  23.5s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.5, clf03__max_iter=5000, total=  23.3s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.5, clf03__max_iter=5000, total=  23.5s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.5, clf03__max_iter=5000, total=  23.9s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=0.5, clf03__max_iter=5000, total=  23.7s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=0.5, clf03__max_iter=10000, total=  47.1s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=0.5, clf03__max_iter=10000, total=  47.3s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=0.5, clf03__max_iter=10000, total=  49.3s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=0.5, clf03__max_iter=10000, total=  51.4s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=0.5, clf03__max_iter=10000, total=  47.1s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.5, clf03__max_iter=20000, total= 1.0min\n",
      "[CV] clf03__C=0.5, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.5, clf03__max_iter=20000, total= 1.1min\n",
      "[CV] clf03__C=0.5, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.5, clf03__max_iter=20000, total=  51.2s\n",
      "[CV] clf03__C=0.5, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.5, clf03__max_iter=20000, total= 1.1min\n",
      "[CV] clf03__C=0.5, clf03__max_iter=20000 .............................\n",
      "[CV] .............. clf03__C=0.5, clf03__max_iter=20000, total= 1.1min\n",
      "[CV] clf03__C=1.0, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=1.0, clf03__max_iter=5000, total=  23.6s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=1.0, clf03__max_iter=5000, total=  24.1s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=1.0, clf03__max_iter=5000, total=  23.4s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=1.0, clf03__max_iter=5000, total=  23.4s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=5000 ..............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............... clf03__C=1.0, clf03__max_iter=5000, total=  23.4s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=10000, total=  47.0s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=10000, total=  46.9s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=10000, total=  46.7s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=10000, total=  47.1s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=10000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=10000, total=  47.0s\n",
      "[CV] clf03__C=1.0, clf03__max_iter=20000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=20000, total= 1.5min\n",
      "[CV] clf03__C=1.0, clf03__max_iter=20000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=20000, total= 1.6min\n",
      "[CV] clf03__C=1.0, clf03__max_iter=20000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=20000, total= 1.7min\n",
      "[CV] clf03__C=1.0, clf03__max_iter=20000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=20000, total= 1.7min\n",
      "[CV] clf03__C=1.0, clf03__max_iter=20000 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsuk/opt/anaconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .............. clf03__C=1.0, clf03__max_iter=20000, total= 1.7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf03', LinearSVC(max_iter=10000))]),\n",
       "             param_grid={'clf03__C': [0.1, 0.5, 1.0],\n",
       "                         'clf03__max_iter': [5000, 10000, 20000]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline03 = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf03', LinearSVC(max_iter=10000))\n",
    "])\n",
    "\n",
    "# Set different parameters for grid search\n",
    "params03 = {\n",
    "    'clf03__C': [0.1, 0.5, 1.0],\n",
    "    'clf03__max_iter': [5000, 10000, 20000],\n",
    "}\n",
    "\n",
    "# Build a pipeline with grid search\n",
    "model03_search = GridSearchCV(pipeline03, params03, verbose=3, cv=3)\n",
    "\n",
    "# Fit the model (takes ~ 30mins)\n",
    "# model03_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 71.86%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72      6811\n",
      "           1       0.72      0.72      0.72      6995\n",
      "\n",
      "    accuracy                           0.72     13806\n",
      "   macro avg       0.72      0.72      0.72     13806\n",
      "weighted avg       0.72      0.72      0.72     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred03c = clf_predict(model03_search.best_estimator_, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred03c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the models folder\n",
    "with open(\"models/clf_linearsvc.pkl\", 'wb') as file:\n",
    "    pickle.dump(model03_search.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "# with open(\"models/clf_linearsvc.pkl\", 'rb') as file:\n",
    "#     model03_search = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the computationally expensive, the model with LinearSVC did not produce satisficatory result. Let's fit the final model with RandomForest classifier given that the best classfier used decision tree up to this point and random forest is a collection of decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5f. Train Random Forest classifier (`model4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model04 = RandomForestClassifier(n_estimators=100) # default\n",
    "model04.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 81.83%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.76      0.81      6811\n",
      "           1       0.79      0.87      0.83      6995\n",
      "\n",
      "    accuracy                           0.82     13806\n",
      "   macro avg       0.82      0.82      0.82     13806\n",
      "weighted avg       0.82      0.82      0.82     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred04 = clf_predict(model04, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier resulted in better results than the default decision tree without any parameter setting, which is promising. Let's now find the best parameters using random forest with Grid Search in the pipeline with standardizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.821, total=   2.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.816, total=   2.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.819, total=   2.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.823, total=   2.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.818, total=   2.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.819, total=   4.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.819, total=   4.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.821, total=   4.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.826, total=   4.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.818, total=   4.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=  11.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.819, total=  11.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.820, total=  11.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.826, total=  11.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=  11.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.824, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.820, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.828, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.828, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.824, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.828, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.820, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.827, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.830, total=   4.0s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.827, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.827, total=  10.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.820, total=  10.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.828, total=  10.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.829, total=  10.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.827, total=  10.1s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.829, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.823, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.829, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.829, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.829, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.822, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.830, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.827, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.824, total=   9.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.823, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.831, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.831, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.830, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.833, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.823, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.832, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.833, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.829, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   8.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.823, total=   8.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.831, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.833, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.829, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.837, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.823, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.830, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.834, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.828, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.834, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.823, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.831, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.834, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.829, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.834, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.823, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.832, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.834, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=1, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.829, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.829, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.823, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.828, total=   2.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.828, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.827, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.828, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.821, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.832, total=   4.3s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.831, total=   4.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.828, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.830, total=  10.3s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.823, total=  10.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.830, total=  10.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.831, total=  10.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.828, total=  10.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.828, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.824, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.831, total=   2.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.830, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.829, total=   2.1s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.829, total=   4.3s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.822, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.828, total=   4.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.829, total=  10.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.823, total=  10.5s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.829, total=  10.5s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=  10.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.828, total=  10.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.831, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.823, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.831, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.829, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.829, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.823, total=   3.8s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   3.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   3.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.828, total=   3.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.832, total=   9.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.822, total=   9.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   9.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.828, total=   9.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.825, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.830, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.832, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.823, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.831, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.834, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.829, total=   3.4s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.833, total=   8.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.823, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.833, total=   8.5s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.829, total=   8.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.834, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.823, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.830, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.833, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.829, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.833, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.823, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.834, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.828, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.835, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.822, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.831, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.833, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=5, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.829, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.831, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.822, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.830, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.831, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.827, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.834, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.821, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.830, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.832, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.829, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.833, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.831, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.831, total=   9.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.829, total=   9.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.832, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.822, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.834, total=   1.9s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.830, total=   2.0s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.827, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.832, total=   3.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.823, total=   3.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.828, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.833, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.821, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.829, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.834, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.823, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.832, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.821, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.830, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.830, total=   3.5s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.833, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.823, total=   8.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.831, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.831, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.829, total=   8.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.833, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.821, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.831, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.829, total=   1.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.834, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.821, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.831, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.834, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.829, total=   3.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.834, total=   8.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.822, total=   8.2s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   8.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.833, total=   8.2s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.829, total=   8.3s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.834, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.823, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.832, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.833, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.827, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.835, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.823, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.831, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.834, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.828, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.834, total=   7.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.823, total=   7.7s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.832, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.832, total=   7.8s\n",
      "[CV] clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=10, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.827, total=   7.8s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.834, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.822, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.830, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.831, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.828, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.835, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.822, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.832, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.829, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.835, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.832, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.832, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.829, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.833, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.821, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.832, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.833, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.827, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.834, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.822, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.828, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.834, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.821, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.832, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.833, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.828, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.832, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.823, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.833, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.828, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.833, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.822, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.832, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.829, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.835, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.823, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.831, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.832, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.829, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.823, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.830, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.832, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.828, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.835, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.822, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.831, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.831, total=   3.1s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.828, total=   3.2s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.834, total=   8.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.823, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.831, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   7.9s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.829, total=   7.8s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.834, total=   1.6s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.822, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.830, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.831, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.827, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.833, total=   3.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.823, total=   3.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.831, total=   3.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.832, total=   3.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.827, total=   3.0s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.834, total=   7.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.822, total=   7.4s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.830, total=   7.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.831, total=   7.5s\n",
      "[CV] clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=20, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.828, total=   7.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.834, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.821, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.830, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.827, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.833, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.823, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.831, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.832, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.826, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.833, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.831, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.831, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.825, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.835, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.823, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.826, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.833, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.822, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.832, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.830, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.826, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.833, total=   7.2s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.821, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.830, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.826, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.833, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.821, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.832, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.830, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.826, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.833, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.821, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.827, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.833, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.821, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.831, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.825, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.834, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.820, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.830, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.827, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.833, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.821, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.833, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.831, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.826, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.833, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.821, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.831, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.826, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.833, total=   1.5s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.822, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.831, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.827, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.834, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.823, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.830, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.831, total=   2.8s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.825, total=   2.9s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.833, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.821, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.831, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.831, total=   7.0s\n",
      "[CV] clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=50, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.826, total=   7.1s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.833, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.821, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.831, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.830, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=100, score=0.826, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.832, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.821, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.831, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.830, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=200, score=0.826, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.833, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.822, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.832, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.829, total=   6.5s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=2, clf04__n_estimators=500, score=0.825, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.833, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.821, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.830, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.830, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=100, score=0.825, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.832, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.823, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.830, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.830, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=200, score=0.825, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.833, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.822, total=   6.5s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.831, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=10, clf04__n_estimators=500, score=0.825, total=   6.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.834, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.822, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.831, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.831, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=100, score=0.825, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.832, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.823, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.832, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.831, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=200, score=0.826, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.833, total=   6.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.821, total=   6.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.831, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.830, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=20, clf04__n_estimators=500, score=0.826, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.833, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.822, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.831, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.829, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=100, score=0.824, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.832, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.822, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.832, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.830, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=200, score=0.826, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.832, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.823, total=   6.5s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.831, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.830, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=50, clf04__n_estimators=500, score=0.826, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.833, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.821, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.832, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.830, total=   1.3s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=100, score=0.825, total=   1.4s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.833, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.822, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.832, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.829, total=   2.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=200, score=0.825, total=   2.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.833, total=   6.7s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.823, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.832, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.829, total=   6.6s\n",
      "[CV] clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500 \n",
      "[CV]  clf04__min_samples_leaf=100, clf04__min_samples_split=100, clf04__n_estimators=500, score=0.824, total=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 450 out of 450 | elapsed: 32.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf04', RandomForestClassifier())]),\n",
       "             param_grid={'clf04__min_samples_leaf': [1, 5, 10, 20, 50, 100],\n",
       "                         'clf04__min_samples_split': [2, 10, 20, 50, 100],\n",
       "                         'clf04__n_estimators': [100, 200, 500]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build pipeline including \n",
    "pipeline04 = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('clf04', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Set different parameters for grid search\n",
    "params04 = {\n",
    "    'clf04__n_estimators': [100,200,500],\n",
    "    'clf04__min_samples_split': [2, 10, 20, 50, 100],\n",
    "    'clf04__min_samples_leaf': [1, 5, 10, 20, 50, 100] \n",
    "}\n",
    "\n",
    "# Build a pipeline with grid search\n",
    "model04_search = GridSearchCV(pipeline04, params04, verbose=3)\n",
    "\n",
    "# Fit the model\n",
    "model04_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 82.83%\n",
      "Classfication report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.81      6811\n",
      "           1       0.80      0.89      0.84      6995\n",
      "\n",
      "    accuracy                           0.83     13806\n",
      "   macro avg       0.83      0.83      0.83     13806\n",
      "weighted avg       0.83      0.83      0.83     13806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred04 = clf_predict(model04_search.best_estimator_, X_test)\n",
    "\n",
    "# Print scores\n",
    "print_classification_scores(y_test, y_pred04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier model using random forest performed just as good as the earlier decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the models folder\n",
    "with open(\"models/clf_randomforest.pkl\", 'wb') as file:\n",
    "    pickle.dump(model04_search.best_estimator_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from file\n",
    "with open(\"models/clf_randomforest.pkl\", 'rb') as file:\n",
    "    temp04 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"work\"></a>\n",
    "[to modules](#modules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
